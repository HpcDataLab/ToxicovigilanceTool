# -*- coding: utf-8 -*-
"""Toxicovigilance tool 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HzN-YHsw6sttTojbSD5O-zIWAbCUeAAF

# Using honeybee flight activity data as a toxicovigilance tool

### **Authors:** 
* Ulises Olivares Pinto<sup>1</sup>, uoliavares@unam.mx
* Alberto Prado Farías<sup>1</sup>, aprado@unam.mx

##### <sup>1</sup> Escuela Nacional de Estudios Superiores Unidad Juriquilla

### 1. Import libraries.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Input, Flatten, Concatenate
from tensorflow.keras import Model
from keras.regularizers import l2
import tensorflow as tf
import os

"""### 2. Import data from Github repository: 

GitHub Repository: https://github.com/HpcDataLab/ToxicovigilanceTool
"""

# Import data from different Datasets
prado = pd.read_csv("https://raw.githubusercontent.com/HpcDataLab/ToxicovigilanceTool/main/data/Daily_flight_activity_Prado.csv")
berascou = pd.read_csv("https://raw.githubusercontent.com/HpcDataLab/ToxicovigilanceTool/main/data/Daily_flight_activity_Berascou.csv")
colin = pd.read_csv("https://raw.githubusercontent.com/HpcDataLab/ToxicovigilanceTool/main/data/Daily_flight_activity_Colin.csv", sep=";")
coulon = pd.read_csv("https://raw.githubusercontent.com/HpcDataLab/ToxicovigilanceTool/main/data/Daily_flight_activity_Coulon.csv")

# Concatenate data
prado = pd.concat([prado, berascou, colin])

print("Number of records:", len(prado))
print("Unique bees: ", len(prado.ID.unique()))

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(prado, prado.Treatment, test_size=0.25, random_state=1234)

print("Shape of data for training:", X_train.shape)
print("Shape of data for testing:", X_test.shape)
print("Unique bees in training data: ", len(X_train.ID.unique()))

# Separate control and pesticide bees
control = prado[prado.Treatment == "control"]
pesticide = prado[prado.Treatment == "pesticide"]

print("Number of control registers: ", len(control))
print("Unique control bees: ", len(control.ID.unique()))
print("Number of pesticide registers: ", len(pesticide))
print("Unique pesticide bees: ", len(pesticide.ID.unique()))

# Preprocess data
Ytrain = pd.get_dummies(y_train).values
Ytest = pd.get_dummies(y_test).values
Xtrain = X_train[["Age", "flights", "duration"]].values.astype('float')
Xtest = X_test[["Age", "flights", "duration"]].values.astype('float')

print("Train DF After drop columns:", Xtrain.shape)
print("Test DF After drop columns:", Xtest.shape)

# Reshape data
Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1]))
Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1]))

print("\Shape of training data:")
print(f"\tRows: {Xtrain.shape[0]}")
print(f"\tColumns: {Xtrain.shape[1]}")
print(f"\tShape of Ytrain: {Ytrain.shape}")

"""### 5. Define a RNN model and compile the RNN."""

def createModel(X_train, Y_train, dropout=0.2, metrics=['accuracy', 'AUC']):
    n_timesteps, n_features = X_train.shape[1], 1

    model = Sequential()
    model.add(Input(shape=(n_timesteps, n_features)))
    model.add(Bidirectional(LSTM(128, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), return_sequences=True)))
    model.add(Dropout(0.1))
    model.add(Bidirectional(LSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01))))
    model.add(Dropout(0.1))
    model.add(Dense(64, kernel_regularizer=l2(0.001), activation='relu'))
    model.add(Dense(2, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=metrics)
    model.summary()

    return model

"""### 6. Train model and get performance metrics."""

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
epochs = 10

model = createModel(X_train=Xtrain, Y_train=Ytrain, dropout=0.2)
history = model.fit(Xtrain, Ytrain, batch_size=512, epochs=epochs, use_multiprocessing=True)

"""### 7. Calculate performance metrics of the model"""

Y_pred = model.predict(Xtest)
Ypred = [np.argmax(i) for i in Y_pred]

Y_real = pd.DataFrame(Ytest).idxmax(axis=1).values

confusion = tf.math.confusion_matrix(Y_real, Ypred)

print(confusion)

import seaborn as sns
sns.heatmap(confusion, annot=True, cmap='Blues', fmt="d")

"""### 8. Plot precision along epochs"""

plt.plot(history.history['accuracy'])
plt.title('Change in Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train'], loc='best')
plt.savefig('Change_in_model_accuracy')
plt.show()

"""### 9. Generate confusion matrix"""

ConfMat = tf.math.confusion_matrix(Y_real, Y_pred).numpy()
print("Confusion Matrix: ")
fig = sns.heatmap(ConfMat, annot=True, center=True, cmap="Oranges", fmt='')
fig.get_figure().savefig("Confusion_Matrix")
